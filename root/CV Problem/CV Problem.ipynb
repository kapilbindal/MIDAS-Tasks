{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import *\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching and visualising Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "with open('train_image.pkl', 'rb') as f: \n",
    "    imgData = pickle.load(f)\n",
    "with open('train_label.pkl', 'rb') as f:\n",
    "    imgLabels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Data\n",
    "with open('test_image.pkl', 'rb') as f:\n",
    "    testData = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 784)\n",
      "(8000,)\n",
      "(2000, 784)\n"
     ]
    }
   ],
   "source": [
    "imgData = np.array(imgData)\n",
    "imgLabels = np.array(imgLabels)\n",
    "testData = np.array(testData)\n",
    "print(imgData.shape)\n",
    "print(imgLabels.shape)\n",
    "print(testData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 3 6]\n",
      "[0 1 2 3]\n"
     ]
    }
   ],
   "source": [
    "# As we are later going to convert labels array to array of One-Hot vectors, so we have to change the labels so as to get max value of label = total no. of classes\n",
    "print(np.unique(imgLabels)) \n",
    "for i in range(imgLabels.shape[0]):\n",
    "    imgLabels[i] = np.ceil(imgLabels[i]/2)\n",
    "print(np.unique(imgLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 28, 28, 1) (8000, 3)\n",
      "(2000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# As Convolutional layer takes image as input, we have to convert given 1D array to 28 X 28 image\n",
    "def preprocessData(X,Y):\n",
    "    X = X.reshape((-1,28,28,1))\n",
    "    X = X/255.0\n",
    "    Y = to_categorical(Y) # Converting yTrain to one Hot vector i.e label 0 will be represented as [1 0]\n",
    "    return X,Y\n",
    "\n",
    "XTrain,YTrain = preprocess_data(data,labels)\n",
    "print(XTrain.shape,YTrain.shape)\n",
    "\n",
    "XTest = testData.reshape((-1,28,28,1))\n",
    "XTest = XTest/255.0\n",
    "print(XTest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "XTrain, XTest, YTrain, YTest = train_test_split(XTrain, YTrain, test_size=0.2, random_state=35, shuffle=True, stratify=YTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-127-c312723c87d0>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-127-c312723c87d0>\"\u001b[1;36m, line \u001b[1;32m5\u001b[0m\n\u001b[1;33m    model.add(Conv2D(64,(3,3),activation='relu'))\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32,(3,3),activation='relu',input_shape=(28,28,1)))\n",
    "# MaxPool layer is used to reduce the dimensions/no. of features and get the feature with max value from the filter window\n",
    "model.add(MaxPool2D((2,2)))\n",
    "model.add(Conv2D(64,(3,3),activation='relu'))\n",
    "model.add(Dropout(0.25)) # It's used to prevent overfitting by randomly removing some neurons in each epoch\n",
    "model.add(MaxPool2D((2,2)))\n",
    "model.add(Conv2D(64,(3,3),activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64,activation='relu'))\n",
    "model.add(Dense(4,activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6120 samples, validate on 680 samples\n",
      "Epoch 1/20\n",
      "6120/6120 [==============================] - 9s 1ms/step - loss: 0.9612 - acc: 0.5895 - val_loss: 0.8345 - val_acc: 0.6206\n",
      "Epoch 2/20\n",
      "6120/6120 [==============================] - 9s 1ms/step - loss: 0.6616 - acc: 0.7059 - val_loss: 0.6664 - val_acc: 0.6926.6742 \n",
      "Epoch 3/20\n",
      "6120/6120 [==============================] - 9s 1ms/step - loss: 0.5623 - acc: 0.7691 - val_loss: 0.5879 - val_acc: 0.7603\n",
      "Epoch 4/20\n",
      "6120/6120 [==============================] - 9s 1ms/step - loss: 0.4988 - acc: 0.7949 - val_loss: 0.5152 - val_acc: 0.7868\n",
      "Epoch 5/20\n",
      "6120/6120 [==============================] - 9s 1ms/step - loss: 0.4545 - acc: 0.8176 - val_loss: 0.5237 - val_acc: 0.7809\n",
      "Epoch 6/20\n",
      "6120/6120 [==============================] - 9s 1ms/step - loss: 0.4318 - acc: 0.8317 - val_loss: 0.4870 - val_acc: 0.8074\n",
      "Epoch 7/20\n",
      "6120/6120 [==============================] - 9s 1ms/step - loss: 0.4093 - acc: 0.8387 - val_loss: 0.4541 - val_acc: 0.8176\n",
      "Epoch 8/20\n",
      "6120/6120 [==============================] - 9s 1ms/step - loss: 0.3816 - acc: 0.8518 - val_loss: 0.4169 - val_acc: 0.8294\n",
      "Epoch 9/20\n",
      "6120/6120 [==============================] - 9s 1ms/step - loss: 0.3674 - acc: 0.8551 - val_loss: 0.4291 - val_acc: 0.8132\n",
      "Epoch 10/20\n",
      "6120/6120 [==============================] - 9s 1ms/step - loss: 0.3470 - acc: 0.8632 - val_loss: 0.4221 - val_acc: 0.8309\n",
      "Epoch 11/20\n",
      "6120/6120 [==============================] - 9s 1ms/step - loss: 0.3344 - acc: 0.8688 - val_loss: 0.4076 - val_acc: 0.8176\n",
      "Epoch 12/20\n",
      "6120/6120 [==============================] - 9s 1ms/step - loss: 0.3245 - acc: 0.8716 - val_loss: 0.4050 - val_acc: 0.8397\n",
      "Epoch 13/20\n",
      "6120/6120 [==============================] - 9s 1ms/step - loss: 0.3072 - acc: 0.8830 - val_loss: 0.3798 - val_acc: 0.8338\n",
      "Epoch 14/20\n",
      "6120/6120 [==============================] - 9s 1ms/step - loss: 0.3007 - acc: 0.8869 - val_loss: 0.4070 - val_acc: 0.8338\n",
      "Epoch 15/20\n",
      "6120/6120 [==============================] - 9s 1ms/step - loss: 0.2918 - acc: 0.8881 - val_loss: 0.4113 - val_acc: 0.8382\n",
      "Epoch 16/20\n",
      "6120/6120 [==============================] - 9s 1ms/step - loss: 0.2644 - acc: 0.9008 - val_loss: 0.3683 - val_acc: 0.8353\n",
      "Epoch 17/20\n",
      "6120/6120 [==============================] - 9s 1ms/step - loss: 0.2550 - acc: 0.9041 - val_loss: 0.3947 - val_acc: 0.8441\n",
      "Epoch 18/20\n",
      "6120/6120 [==============================] - 9s 1ms/step - loss: 0.2449 - acc: 0.9060 - val_loss: 0.4054 - val_acc: 0.8456\n",
      "Epoch 19/20\n",
      "6120/6120 [==============================] - 9s 1ms/step - loss: 0.2274 - acc: 0.9149 - val_loss: 0.3720 - val_acc: 0.8412\n",
      "Epoch 20/20\n",
      "6120/6120 [==============================] - 9s 1ms/step - loss: 0.2129 - acc: 0.9208 - val_loss: 0.3970 - val_acc: 0.8412\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "hist = model.fit(X_train,Y_train,epochs=20,validation_split=0.1,batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200/1200 [==============================] - 1s 431us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3758394642670949, 0.8666666666666667]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
